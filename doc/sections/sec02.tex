\section{Methods}
\label{sec:two}

We employ multiple different methods for the problems of feature selection,
regression and classification.  Models for the latter two tasks are tested
against a separate baseline.

In the following sections we describe these methods in detail.

\subsection{Feature Selection}

Select features based on highest scores:

\begin{itemize}
\item f\_regression (F-value between label/feature for regression tasks)
\item chi2 (Chi-squared stats of non-negative features for classification tasks)
\end{itemize}

\subsection{Regression}

In this section, we discuss methods applied to solve the regression task.

We choose a function as a baseline that always returns the mean of outcomes in
the training data for all observations, i.e., the mean of a number of votes in
the training data.

In order to compare to and improve the baseline performance, we implement linear
regression and polynomial regression of degree two.  Both methods are solved by
applying the solution of the ordinary least squares (OLS) problem.

\subsubsection{Linear Regression}

Linear regression models the relationship between the features and the outcome
using a linear function; in this case a linear relationship between vectors of
word counts (features) and the number of votes (outcome).  This relationship can
be expressed as
\[
  y = \mathbf{x}^T\mathbf{w}
\]

The weight vector $\mathbf{w}$ contains the weight associated with each feature
in $\mathbf{x}$ to predict an outcome $y$.

As we want to model the relationship provided by the training data, the reviews
(word count column vectors) can be written as a matrix $\mathbf{X}$, where in
turn each row is a vector of features describing a single review.  The number of
votes for each review can be written as a column vector $\mathbf{y}$.  We
formalize the relationship as
\[
  \mathbf{y} = \mathbf{X}^T \mathbf{w}
\]

Consequently, we apply the OLS method to find a solution for the weight vector
$\mathbf{w}$.
\[
  \mathbf{w} = {(\mathbf{X}^T \mathbf{X})}^{-1} \mathbf{X}^T \mathbf{y}
\]

\subsubsection{Polynomial Regression}

Polynomial regression provides a model, where the relationship between the
features and the outcome is nonlinear.  Polynomial regression with degree two
expands the original set of features with the two-way interaction between those
features and a set of features with a power of two.  This interaction of
features improves modeling the nonlinear relationship.

For example, given a feature vector $\mathbf{x} = (x_1, x_2)$, after expanding
the set of features as described above, our new feature vector becomes
$\mathbf{x}^{'} = (x_1, x_2, x_1x_2, x_1^2, x_2^2)$.

We treat the new set of features as if they were independent from each other
and apply the OLS method to estimate the outcome.

\subsection{Classification}

This section is concerned with the methods applied in the classification task.

We chose as a baseline a function, which always predicts the majority label,
that is, the label for the most probable class in the outcomes of the training
data.  When using a logarithmic loss as a measurement function, the value is the
probability of the most probable class.  Alternatively, when using other scoring
functions such as accuracy or \fmeasure, the value becomes the label of the most
probable class.

We implement a $k$-Nearest Neighbor ($k$-NN) classifier and compare its
performance to the baseline and a random forest algorithm.

\subsubsection{k-Nearest Neighbor (k-NN)}

\subsubsection{Random Forest}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
