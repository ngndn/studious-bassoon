\section{Methods}
\label{sec:two}

We employ multiple different methods for the problems of feature selection,
regression and classification.  Models for the latter two tasks are tested
against a separate baseline.

In the following section we describe these methods in detail.

\subsection{Feature Selection}

The data set contains a large number of vectors with counts for presumably
relevant words.  This relevancy can be quantified based on univariate
statistical tests.  We utilize two different feature scoring functions:

\begin{description}
\item[Linear Regression Test] A linear model, which computes the cross correlation
  between each regressor and the target outcome, and converting the result to
  F-values.
\item[Chi-Squared Test] Computes $\chi^2$ statistics for all features,
  indicating the discriminate power of each.
\end{description}

The above tests provide a list of scores for all 50 features for the regression
and classification tasks, respectively.  These lists, when sorted, comprise the
top-$k$ sets used to evaluate the model performance in our experiment.  Before
fitting each model to the individual data, we select the top-$k$ features ($k =
\{1, \ldots, 50\}$), and discard the rest.  This process is explained in more
detail in Section~\ref{sec:three}.

\subsection{Regression}

In this section, we discuss methods applied to solve the regression task.

We choose a function as a baseline that always returns the mean of outcomes in
the training data for all observations, i.e., the mean of a number of votes in
the training data.

In order to compare to and improve the baseline performance, we implement linear
regression and polynomial regression of degree two.  Both methods are solved by
applying the solution of the ordinary least squares (OLS) problem.

\subsubsection{Linear Regression}

Linear regression models the relationship between the features and the outcome
using a linear function; in this case a linear relationship between vectors of
word counts (features) and the number of votes (outcome).  This relationship can
be expressed as

\[
  y = \mathbf{x}^T\mathbf{w}
\]

The weight vector $\mathbf{w}$ contains the weight associated with each feature
in $\mathbf{x}$ to predict an outcome $y$.

As we want to model the relationship provided by the training data, the reviews
(word count column vectors) can be written as a matrix $\mathbf{X}$, where in
turn each row is a vector of features describing a single review.  The number of
votes for each review can be written as a column vector $\mathbf{y}$.  We
formalize the relationship as

\[
  \mathbf{y} = \mathbf{X}^T \mathbf{w}
\]

Consequently, we apply the OLS method to find a solution for the weight vector
$\mathbf{w}$.

\[
  \mathbf{w} = {(\mathbf{X}^T \mathbf{X})}^{-1} \mathbf{X}^T \mathbf{y}
\]

\subsubsection{Polynomial Regression}

Polynomial regression provides a model, where the relationship between the
features and the outcome is nonlinear.  Polynomial regression with degree two
expands the original set of features with the two-way interaction between those
features and a set of features with a power of two.  This interaction of
features improves modeling the nonlinear relationship.

For example, given a feature vector $\mathbf{x} = (x_1, x_2)$, after expanding
the set of features as described above, our new feature vector becomes
$\mathbf{x}^{'} = (x_1, x_2, x_1x_2, x_1^2, x_2^2)$.

We treat the new set of features as if they were independent from each other
and apply the OLS method to estimate the outcome.

\subsection{Classification}

This section is concerned with the methods applied in the classification task.

We chose as a baseline a function, which always predicts the majority label,
that is, the label for the most probable class in the outcomes of the training
data.  When using a logarithmic loss as a measurement function, the value is the
probability of the most probable class.  Alternatively, when using other scoring
functions such as accuracy or \fmeasure, the value becomes the label of the most
probable class.

We implement a $k$-Nearest Neighbor ($k$-NN) classifier and compare its
performance to the baseline and a random forest algorithm.

\subsubsection{$k$-Nearest Neighbor ($k$-NN)}

$k$-NN is a non-parametric classification method, i.e, it takes the whole
training data as a model and grows with increasing data.  For predicting a label
for a new observation, $k$-NN first computes the distances between that
observation and all observations in the training data.  Then, from the top-$k$
nearest observations the majority label (or probability of the majority label)
is returned.

Since $k$-NN considers all training data when predicting a label, it does not
require model fitting (training).  However, we need to pick the value of $k$ for
the model.  Here $k$ is the hyper parameter of the model.  The value of $k$ is
often chosen by running cross-validation on the training data, which we discuss
in detail in secion~\ref{sec:three}.

\subsubsection{Random Forest}

Random forest\cite{DBLP:journals/ml/Breiman01} is an ensemble learning method, which can be used for
classification or regression tasks.  Model fitting is done by training many
decision trees using random samples from the training data.  This technique is
also known as bootstrap aggregating or bagging.  After training, the model
utilizes the trees to choose a label for a new observations by averaging the
results from trees.

A decision tree is a popular basic model in machine learning.  A major problem
with decision trees is that growing a very deep tree tends to fit noise in the
training data.  Deep trees have low bias but high variance.  Averaging results
from different trees, which are trained on different data set samples (like in
random forests) reduces the variance.

However, applying bagging on multiple trees and averaging the results does not
necessarily reduce high variance, if these trees are highly correlated.  Random
forest additionally employs feature bagging during the training.  Here, each
tree will train on $m$ randomly selected features (with replacement) from the
original set of $M$ features.  This effectively reduces number of times highly
predictive features appear in the trained trees, preventing high correlation
between them.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
