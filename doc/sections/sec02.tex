\section{Methods}
\label{sec:two}

We employ multiple different methods for the problems of feature selection,
regression and classification.  Models for the latter two tasks are tested
against a separate baseline.

In the following sections we describe these methods in detail.

\subsection{Feature Selection}

Select features based on highest scores:

\begin{itemize}
\item f\_regression (F-value between label/feature for regression tasks)
\item chi2 (Chi-squared stats of non-negative features for classification tasks)
\end{itemize}

\subsection{Regression}

In this section, we discuss methods applied to solve the regression task.

We choose a function as a baseline that always returns the mean of outcomes in
the training data for all observations, i.e., the mean of a number of votes in
the training data.

In order to compare to and improve the baseline performance, we implement linear
regression and polynomial regression of degree two.  Both methods are solved by
applying the solution of the ordinary least squares (OLS) problem.

\subsubsection{Linear Regression}

Linear regression models the relationship between the features and the outcome
using a linear function; in this case a linear relationship between vectors of
word counts (features) and the number of votes (outcome).  This relationship can
be expressed as
\[
  y = \mathbf{x}^T\mathbf{w}
\]

The weight vector $\mathbf{w}$ contains the weight associated with each feature
in $\mathbf{x}$ to predict an outcome $y$.

As we want to model the relationship provided by the training data, the reviews
(word count column vectors) can be written as a matrix $\mathbf{X}$, where in
turn each row is a vector of features describing a single review.  The number of
votes for each review can be written as a column vector $\mathbf{y}$.  We
formalize the relationship as
\[
  \mathbf{y} = \mathbf{X}^T \mathbf{w}
\]

Consequently, we apply the OLS method to find a solution for the weight vector
$\mathbf{w}$.
\[
  \mathbf{w} = {(\mathbf{X}^T \mathbf{X})}^{-1} \mathbf{X}^T \mathbf{y}
\]

\subsubsection{Polynomial Regression}

Polynomial regression provides a model, where the relationship between the
features and the outcome is nonlinear.  Polynomial regression with degree two
expands the original set of features with the two-way interaction between those
features and a set of features with a power of two.  This interaction of
features improves modeling the nonlinear relationship.

For example, given a feature vector $\mathbf{x} = (x_1, x_2)$, after expanding
the set of features as described above, our new feature vector becomes
$\mathbf{x}^{'} = (x_1, x_2, x_1x_2, x_1^2, x_2^2)$.

We treat the new set of features as if they were independent from each other
and apply the OLS method to estimate the outcome.

\subsection{Classification}

This section is concerned with the methods applied in the classification task.

We chose as a baseline a function, which always predicts the majority label,
that is, the label for the most probable class in the outcomes of the training
data.  When using a logarithmic loss as a measurement function, the value is the
probability of the most probable class.  Alternatively, when using other scoring
functions such as accuracy or \fmeasure, the value becomes the label of the most
probable class.

We implement a $k$-Nearest Neighbor ($k$-NN) classifier and compare its
performance to the baseline and a random forest algorithm.

\subsubsection{k-Nearest Neighbor (k-NN)}
k-NN is a non-parametric classification method, i.e, k-NN takes the whole training data as a model and it will grow as the training data grows. For predicting label for new observation, k-NN first computes the distances between new observation and all observations in the training data. It then picks the top-k nearest observations and returns the majority label among them (or probability of the majority label).

Since the k-NN takes all the training data to be the model, k-NN does not require training. However, we do need to choose the value of k for the model. The value of k is often chosen by running cross validation on the training data. Detail will be discussed in Experiments section.

\subsubsection{Random Forest}
Random Forest is an ensemble learning method which can be used for classification or regression task. Random forest does training by train many decision trees using different parts of the training data. This technique is also called bootstrap aggregating or bagging. Later, random forest will use those trees to make decision about new observation by averaging the results from trees.

Decision tree is one of the popular and basic methods in machine learning. One problem in decision tree is that growing a very deep tree tends to fit noise in the training data. Deep tree has low bias but high variance. Averaging results from different trees training on different parts of the data, as in random forest, will reduce the variance.

However, only applying bagging on multiple trees then averaging the result is not necessary reducing the variance if those trees are highly correlated. Random forest also applies a technique called feature bagging when training decision trees. With feature bagging, each tree will train on $m$ different features randomly selected (with replacement) from $M$ original features. This technique prevent a highly predictive feature in a training set appears many times in trees, preventing those trees to be highly correlated.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
