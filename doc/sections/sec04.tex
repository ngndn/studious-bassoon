\section{Results}
\label{sec:four}
\subsection{Regression}
We now present the results for regression and classification task.

In regression, we run cross validation for three methods: baseline, linear regression, and polynomial with degree two for different sets of top-k features. We use MSE to measure the performance. The results are presented in the left graph of Figure~\ref{fig:training}. 

Both linear regression can easily overcome the baseline. Polynomial regression perform a little bit better than linear regression. The performances become better if we use more features. However, using too many features does not help because of correlation between them. 

Based on those results, using 10 selected features help linear regression achieve the best result. Meanwhile, 7 selected features are enough for polynomial regression. In practice, we will choose polynomial regression as our final model and report only one result from it. Here, for demonstration purpose, we also run baseline and linear regression with the best setting on testing data. The results are presented in Table~\ref{tab:regperf}.

As we can see, polynomial regression still maintains a good performance compare to linear regression and beats the baseline by a significant amount.

\begin{table}[t]
  \caption{Regression performance comparison of (1) baseline, (2) linear
    regression and (3) polynomial regression of degree two as measured using the
    mean squared error on the test data set.}
  % \centering\small
  % \renewcommand{\tabcolsep}{1pt}
  \newcolumntype{C}{>{\centering\arraybackslash}X}%
  % \newcolumntype{R}{>{\raggedleft\arraybackslash}X}
  \begin{tabularx}{\linewidth}{@{\kern3pt}cCc@{\kern3pt}}
    \toprule
    \bfseries Model & \bfseries Top-$k$ Features & \bfseries MSE \\
    \midrule
    (1) & --- &  12.60835 \\
    (2) &  10 &  0.047144 \\
    (3) &  7  &  0.032311 \\
    \bottomrule
  \end{tabularx}
\label{tab:regperf}
\end{table}

\subsection{Classification}


\begin{table}[t]
  \caption{Classification performance comparison of (1) baseline, $k$-NN with
    (2) 1 and (3) 5 neighbors, random forest with (4) 10 and (5) 50 decision
    trees as measured using the \fmeasure{} on the test data set.}
  % \centering\small
  % \renewcommand{\tabcolsep}{1pt}
  \newcolumntype{C}{>{\centering\arraybackslash}X}%
  % \newcolumntype{R}{>{\raggedleft\arraybackslash}X}
  \begin{tabularx}{\linewidth}{@{\kern3pt}cCc@{\kern3pt}}
    \toprule
    \bfseries Model & \bfseries Top-$k$ Features & \bfseries \fmeasure{} \\
    \midrule
    (1) & --- & 0.771499 \\
    (2) &   4 & 0.805405 \\
    (3) &   4 & 0.799732 \\
    (4) &   6 & 0.812203 \\
    (5) &   6 & 0.814915 \\
    \bottomrule
  \end{tabularx}
\label{tab:clsperf}
\end{table}

\begin{figure*}[h]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/reg_mse_train.png}
\end{minipage}
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/cls_fscore_train.png}
\end{minipage}
\caption{Performance graphs for regression (left) and classification (right) for
  number of top-$k$ features, evaluated with MSE and \fmeasure, respectively.}
\label{fig:training}
\end{figure*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
